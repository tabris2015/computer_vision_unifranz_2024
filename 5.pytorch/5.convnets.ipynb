{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mejorando la Precisión a través de convoluciones\n",
    "\n",
    "En el anterior notebook hemos visto cómo implementar una red neuronal profunda con 3 capas, una capa de entrada, una capa oculta y la capa de salida. También hemos experimentado usando distinto número de unidades y capas ocultas para observar su efecto en la precisión del modelo entrenado.\n",
    "\n",
    "Para una referencia conveniente, a continuación tenemos el código completo para tener un valor inicial de precisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando: cuda\n"
     ]
    }
   ],
   "source": [
    "# codigo visto previamente\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "# dispositivo de entrenamiento\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Usando: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))  # Example normalization for single-channel (grayscale) images\n",
    "])\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=256, num_workers=16)\n",
    "test_dataloader = DataLoader(test_data, batch_size=256, num_workers=16)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# bucle de entrenamiento\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # modo entrenamiento\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # mover si es necesario\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # forward\n",
    "        # prediccion y costo\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# funcion de pruebas\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # modo evaluacion\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    # evaluacion del modelo con torch.no_grad\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.316878  [   64/60000]\n",
      "loss: 2.295246  [ 6464/60000]\n",
      "loss: 2.287181  [12864/60000]\n",
      "loss: 2.278426  [19264/60000]\n",
      "loss: 2.255558  [25664/60000]\n",
      "loss: 2.257093  [32064/60000]\n",
      "loss: 2.247988  [38464/60000]\n",
      "loss: 2.235126  [44864/60000]\n",
      "loss: 2.249377  [51264/60000]\n",
      "loss: 2.207842  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 2.208562 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.240923  [   64/60000]\n",
      "loss: 2.208345  [ 6464/60000]\n",
      "loss: 2.177410  [12864/60000]\n",
      "loss: 2.175064  [19264/60000]\n",
      "loss: 2.125364  [25664/60000]\n",
      "loss: 2.123140  [32064/60000]\n",
      "loss: 2.128568  [38464/60000]\n",
      "loss: 2.088676  [44864/60000]\n",
      "loss: 2.113034  [51264/60000]\n",
      "loss: 2.022747  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.023179 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.081243  [   64/60000]\n",
      "loss: 2.017016  [ 6464/60000]\n",
      "loss: 1.950933  [12864/60000]\n",
      "loss: 1.961246  [19264/60000]\n",
      "loss: 1.845762  [25664/60000]\n",
      "loss: 1.835933  [32064/60000]\n",
      "loss: 1.845186  [38464/60000]\n",
      "loss: 1.765807  [44864/60000]\n",
      "loss: 1.799837  [51264/60000]\n",
      "loss: 1.664401  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 1.683047 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.764410  [   64/60000]\n",
      "loss: 1.692216  [ 6464/60000]\n",
      "loss: 1.594025  [12864/60000]\n",
      "loss: 1.620313  [19264/60000]\n",
      "loss: 1.479866  [25664/60000]\n",
      "loss: 1.492581  [32064/60000]\n",
      "loss: 1.483858  [38464/60000]\n",
      "loss: 1.415720  [44864/60000]\n",
      "loss: 1.453824  [51264/60000]\n",
      "loss: 1.326551  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.363042 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.459304  [   64/60000]\n",
      "loss: 1.406794  [ 6464/60000]\n",
      "loss: 1.284908  [12864/60000]\n",
      "loss: 1.340938  [19264/60000]\n",
      "loss: 1.213295  [25664/60000]\n",
      "loss: 1.246836  [32064/60000]\n",
      "loss: 1.238581  [38464/60000]\n",
      "loss: 1.184917  [44864/60000]\n",
      "loss: 1.232177  [51264/60000]\n",
      "loss: 1.131420  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.157914 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.252484  [   64/60000]\n",
      "loss: 1.222076  [ 6464/60000]\n",
      "loss: 1.076181  [12864/60000]\n",
      "loss: 1.172734  [19264/60000]\n",
      "loss: 1.046396  [25664/60000]\n",
      "loss: 1.083794  [32064/60000]\n",
      "loss: 1.095592  [38464/60000]\n",
      "loss: 1.040039  [44864/60000]\n",
      "loss: 1.093801  [51264/60000]\n",
      "loss: 1.019666  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.030981 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.113830  [   64/60000]\n",
      "loss: 1.107735  [ 6464/60000]\n",
      "loss: 0.938375  [12864/60000]\n",
      "loss: 1.067915  [19264/60000]\n",
      "loss: 0.944920  [25664/60000]\n",
      "loss: 0.973268  [32064/60000]\n",
      "loss: 1.009464  [38464/60000]\n",
      "loss: 0.948684  [44864/60000]\n",
      "loss: 1.001598  [51264/60000]\n",
      "loss: 0.950194  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.948441 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.014928  [   64/60000]\n",
      "loss: 1.032757  [ 6464/60000]\n",
      "loss: 0.843297  [12864/60000]\n",
      "loss: 0.996299  [19264/60000]\n",
      "loss: 0.880377  [25664/60000]\n",
      "loss: 0.894219  [32064/60000]\n",
      "loss: 0.952388  [38464/60000]\n",
      "loss: 0.888933  [44864/60000]\n",
      "loss: 0.936574  [51264/60000]\n",
      "loss: 0.901986  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.890796 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 8\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# bucle principal\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cómo podemos mejorar este rendimiento?\n",
    "\n",
    "Una forma es usar algo llamado Convolución. Una Convolución es una operación matemática que puede ser aplicada a señales de distinta naturaleza, en este caso, la señal a aplicar será nuestra imagen y sus correspondientes mapas de características en las capas ocultas.\n",
    "\n",
    "Pero antes de poder aplicar esta operación, debemos definir de mejor manera qué es y cómo se usa una convolución en imágenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convoluciones\n",
    "En escencia, tendremos un arreglo llamado **parche** o **kernel** (usualmente de 3x3 o 5x5) y lo *deslizaremos* sobre la imagen en la que queremos operar. Se puede observar una visualización de la misma operación en la siguiente figura:\n",
    "\n",
    "![tensor](conv1.gif)\n",
    "\n",
    "en el caso anterior, un parche de 3x3 se desliza sobre una imagen original de tamaño 5x5 obteniendo como resultado una **nueva imagen** de tamaño 5x5, nótese que para que podamos tener una imagen del mismo tamaño original necesitamos agregar valores *extra* en los extremos de la imagen de entrada, esta técnica se denomina *padding*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels de convolución\n",
    "\n",
    "En el contexto de las redes neuronales, intercambiaremos en nuestra arquitectura la estructura tradicional de las **neuronas** o unidades en las capas de la red por **filtros**. En la siguiente figura se puede visualizar el efecto de estos filtros.\n",
    "\n",
    "![tensor](conv3.gif)\n",
    "\n",
    "Por tanto, durante el entrenamiento, la tarea es encontrar los valores más adecuados de cada elemento de los filtros para minimizar la función de costo o pérdida.\n",
    "\n",
    "La naturaleza y características de la operación de convolución son ideales para ser implementadas en tareas de visión artificial pues permiten a la red, a través de los filtros entrenados, *resaltar* las características más importantes para la predicción y mejora la eficiencia computacional pues nos enfocamos solamente en entrenar sobre ésas mismas características resaltadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST con una red convolucional\n",
    "\n",
    "Una vez entendido el concepto e importancia de la operación de convolución podemos aplicarlo a nuestra tarea de clasificación de prendas de vestir.\n",
    "\n",
    "Pyotch nos permite definir una capa especial llamada **Conv2D** que implementa todas las operaciones.\n",
    "\n",
    "La implementación mejorada se vería así:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (backbone): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=800, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=800, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        # flatten tensor\n",
    "        flat_features = features.view(features.size(0), -1)\n",
    "        # apply the classifier\n",
    "        logits = self.head(flat_features)\n",
    "        return logits\n",
    "    \n",
    "model2 = ConvNet().to(device)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304158  [  256/60000]\n",
      "loss: 1.527696  [25856/60000]\n",
      "loss: 0.888424  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.826331 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.792175  [  256/60000]\n",
      "loss: 0.695051  [25856/60000]\n",
      "loss: 0.625680  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.660008 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.600452  [  256/60000]\n",
      "loss: 0.576911  [25856/60000]\n",
      "loss: 0.548563  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.591401 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.522961  [  256/60000]\n",
      "loss: 0.514955  [25856/60000]\n",
      "loss: 0.511449  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.551093 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.480779  [  256/60000]\n",
      "loss: 0.473875  [25856/60000]\n",
      "loss: 0.487611  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.520588 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.449322  [  256/60000]\n",
      "loss: 0.444753  [25856/60000]\n",
      "loss: 0.472115  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.497652 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.423744  [  256/60000]\n",
      "loss: 0.420103  [25856/60000]\n",
      "loss: 0.459011  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.478824 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.401688  [  256/60000]\n",
      "loss: 0.400219  [25856/60000]\n",
      "loss: 0.448258  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.463285 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 256\n",
    "epochs = 8\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "# bucle principal\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model2, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analice la diferencia en la precisión obtenida con nuestra nueva red convolucional.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios de exploración\n",
    "\n",
    "Incremente el número de épocas en el entrenamiento y analice cuidadosamente el efecto en la precisión final tanto en el conjunto de entrenamiento como en el conjunto de validación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando las capas convolucionales\n",
    "\n",
    "Visualizaremos los efectos de la convolución y los **mapas de características** de las capas ocultas de manera visual. \n",
    "\n",
    "De los 100 primeros ejemplos en el conjunto de pruebas, aquellos entre el índice 23 y 28 son zapatos. Analice la similaridad entre sus mapas de características.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = []\n",
    "# Define the hook function\n",
    "def hook_fn(module, input, output):\n",
    "    print(\"hooking...\")\n",
    "    feature_maps.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register hook to the layers you want to visualize\n",
    "for layer in [model2.backbone[0], model2.backbone[3]]:  # Conv layers at index 0 and 3\n",
    "    layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Define the visualization function\n",
    "def visualize_feature_maps(feature_maps, num_cols=8):\n",
    "    for i, feature_map in enumerate(feature_maps):\n",
    "        num_filters = feature_map.shape[1]\n",
    "        num_rows = (num_filters + num_cols - 1) // num_cols  # Ensure enough rows\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n",
    "        \n",
    "        for j in range(num_filters):\n",
    "            row = j // num_cols\n",
    "            col = j % num_cols\n",
    "            ax = axes[row, col]\n",
    "            ax.imshow(feature_map[0, j].detach().cpu().numpy(), cmap='viridis')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        for j in range(num_filters, num_rows * num_cols):  # Hide unused subplots\n",
    "            row = j // num_cols\n",
    "            col = j % num_cols\n",
    "            axes[row, col].axis('off')\n",
    "\n",
    "        fig.suptitle(f'Feature Map {i+1}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = training_data[10]\n",
    "image = image.to(device)\n",
    "label = torch.tensor(label).to(device)\n",
    "# Clear previous feature maps\n",
    "feature_maps.clear()\n",
    "output = model2(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_feature_maps([fm for fm in feature_maps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "1. Modifique la cantidad de filtros de convolución, cambie de 32 a 16 o 64, cuál es el impacto en la precisión y/o el tiempo de entrenamiento?\n",
    "2. Elimine la última capa de convolución, cuál es el impacto?\n",
    "3. Y qué pasa si se agregan nuevas capas de convolución?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62eb3e910e44ea0e9978ea29c6f3fc7540fb99bfa181faf1a80d42ed442aa249"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('unifranz': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
