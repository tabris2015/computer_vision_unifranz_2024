{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento completo\n",
    "Hemos visto los diferentes componentes y pre requisitos de código para comenzar a entrenar nuestras redes neuronales. Ahora pondremos todos los aspectos juntos. El entrenamiento de un modelo es un proceso iterativo. En cada iteración se debe seguir los siguientes pasos:\n",
    "\n",
    "1. Recolectar muestras\n",
    "2. hacer una predicción\n",
    "3. calcular el error de la predicción (loss)\n",
    "4. calcular la gradiente del error con respecto de los parámetros\n",
    "5. modificar los parámetros de acuerdo a un criterio de **optimización**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codigo visto previamente\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del modelo de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiperparámetros\n",
    "\n",
    "Los hiperparámetros son parámetros ajustables que nos permiten controlar el proceso de optimización y entrenamiento del modelo. La variación en los valores de los hiperparámetros impactan en el entrenamiento y los radios de convergencia. Definiremos los siguientes hiperparámetros para este ejemplo:\n",
    "\n",
    "- Número de Épocas: El número de veces de iterar sobre todo el dataset de entrenamiento.\n",
    "- Batch Size: Tamaño del minilote. El número de muestras propagadas a través de la red antes de ser actualizados los parámetros de la red.\n",
    "- Learning Rate: Un parámetro de cantidad de variación de parámetros en cada lote/época. Valores pequeños hacen un entrenamiento lento, valores muy grandes pueden hacer el entrenamiento inestable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucle de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos los dataset, definición del modelo y los hiperparámetros, podemos proceder a correr el bucle de entrenamiento. Cada iteración del bucle se conoce como Época. En cada época realizamos 2 procesos:\n",
    "\n",
    "1. Train Loop: El bucle de entrenamiento y optimización de los parámetros de la red (Backpropagation).\n",
    "2. Validation/Test Loop: Bucle para recolectar métricas de rendimiento durante el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de costo\n",
    "Las funciones de costo miden el grado de similitud entre las predicciones de la red y los resultados esperados. La idea principal del entrenamiento de una red es minimizar la función de costo. \n",
    "\n",
    "Pytorch incluye implementaciones de funciones de costo comunes como MSE, NLL, CrossEntropy y otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizador\n",
    "La optimizaci'on es el proceso de ajustar los parámetros del modelo para reducir el error en cada paso de entrenamiento. Los algoritmos de optimización u Optimizadores definen la forma en la que este proceso es realizado. Toda la lógica de optimización esta encapsulada en el objeto `optimizer`. \n",
    "\n",
    "Pytorch también incluye implementaciones de optimizadores populares como ADAM, RMSProp, SGD y otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemetanción completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucle de entrenamiento\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # modo entrenamiento\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward\n",
    "        # prediccion y costo\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# funcion de pruebas\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # modo evaluacion\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    # evaluacion del modelo con torch.no_grad\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.308186  [   64/60000]\n",
      "loss: 2.291214  [ 6464/60000]\n",
      "loss: 2.275085  [12864/60000]\n",
      "loss: 2.264726  [19264/60000]\n",
      "loss: 2.250510  [25664/60000]\n",
      "loss: 2.221443  [32064/60000]\n",
      "loss: 2.228258  [38464/60000]\n",
      "loss: 2.193028  [44864/60000]\n",
      "loss: 2.188263  [51264/60000]\n",
      "loss: 2.149150  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 2.151133 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.162347  [   64/60000]\n",
      "loss: 2.154422  [ 6464/60000]\n",
      "loss: 2.095660  [12864/60000]\n",
      "loss: 2.107693  [19264/60000]\n",
      "loss: 2.062939  [25664/60000]\n",
      "loss: 1.998299  [32064/60000]\n",
      "loss: 2.018794  [38464/60000]\n",
      "loss: 1.936786  [44864/60000]\n",
      "loss: 1.940431  [51264/60000]\n",
      "loss: 1.863470  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.867706 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.901198  [   64/60000]\n",
      "loss: 1.878707  [ 6464/60000]\n",
      "loss: 1.754277  [12864/60000]\n",
      "loss: 1.789234  [19264/60000]\n",
      "loss: 1.690365  [25664/60000]\n",
      "loss: 1.638416  [32064/60000]\n",
      "loss: 1.644914  [38464/60000]\n",
      "loss: 1.547981  [44864/60000]\n",
      "loss: 1.575200  [51264/60000]\n",
      "loss: 1.468739  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.490385 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.556690  [   64/60000]\n",
      "loss: 1.530569  [ 6464/60000]\n",
      "loss: 1.373137  [12864/60000]\n",
      "loss: 1.445733  [19264/60000]\n",
      "loss: 1.336638  [25664/60000]\n",
      "loss: 1.329434  [32064/60000]\n",
      "loss: 1.334006  [38464/60000]\n",
      "loss: 1.259106  [44864/60000]\n",
      "loss: 1.299885  [51264/60000]\n",
      "loss: 1.205450  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 1.228687 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.302536  [   64/60000]\n",
      "loss: 1.292154  [ 6464/60000]\n",
      "loss: 1.120144  [12864/60000]\n",
      "loss: 1.230082  [19264/60000]\n",
      "loss: 1.109377  [25664/60000]\n",
      "loss: 1.133772  [32064/60000]\n",
      "loss: 1.148979  [38464/60000]\n",
      "loss: 1.084658  [44864/60000]\n",
      "loss: 1.129734  [51264/60000]\n",
      "loss: 1.053952  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 1.068945 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.135311  [   64/60000]\n",
      "loss: 1.144495  [ 6464/60000]\n",
      "loss: 0.956255  [12864/60000]\n",
      "loss: 1.097143  [19264/60000]\n",
      "loss: 0.970026  [25664/60000]\n",
      "loss: 1.003752  [32064/60000]\n",
      "loss: 1.034575  [38464/60000]\n",
      "loss: 0.975023  [44864/60000]\n",
      "loss: 1.018594  [51264/60000]\n",
      "loss: 0.958607  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.965930 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.018659  [   64/60000]\n",
      "loss: 1.048849  [ 6464/60000]\n",
      "loss: 0.844084  [12864/60000]\n",
      "loss: 1.008930  [19264/60000]\n",
      "loss: 0.881086  [25664/60000]\n",
      "loss: 0.912364  [32064/60000]\n",
      "loss: 0.958053  [38464/60000]\n",
      "loss: 0.904226  [44864/60000]\n",
      "loss: 0.941125  [51264/60000]\n",
      "loss: 0.893692  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.895211 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.931927  [   64/60000]\n",
      "loss: 0.981789  [ 6464/60000]\n",
      "loss: 0.763346  [12864/60000]\n",
      "loss: 0.946197  [19264/60000]\n",
      "loss: 0.821079  [25664/60000]\n",
      "loss: 0.845762  [32064/60000]\n",
      "loss: 0.902959  [38464/60000]\n",
      "loss: 0.857325  [44864/60000]\n",
      "loss: 0.884950  [51264/60000]\n",
      "loss: 0.846435  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.844207 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.865246  [   64/60000]\n",
      "loss: 0.931092  [ 6464/60000]\n",
      "loss: 0.702869  [12864/60000]\n",
      "loss: 0.899590  [19264/60000]\n",
      "loss: 0.778235  [25664/60000]\n",
      "loss: 0.795940  [32064/60000]\n",
      "loss: 0.860633  [38464/60000]\n",
      "loss: 0.824698  [44864/60000]\n",
      "loss: 0.842997  [51264/60000]\n",
      "loss: 0.809785  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.805502 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.812095  [   64/60000]\n",
      "loss: 0.890218  [ 6464/60000]\n",
      "loss: 0.655830  [12864/60000]\n",
      "loss: 0.863713  [19264/60000]\n",
      "loss: 0.745730  [25664/60000]\n",
      "loss: 0.757696  [32064/60000]\n",
      "loss: 0.826273  [38464/60000]\n",
      "loss: 0.800359  [44864/60000]\n",
      "loss: 0.810345  [51264/60000]\n",
      "loss: 0.780108  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.774712 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# bucle principal\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_unifranz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
