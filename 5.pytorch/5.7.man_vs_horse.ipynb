{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humanos vs Caballos\n",
    "\n",
    "En los anteriores ejercicios hemos podido ver a las redes neuronales convolucionales en acción y cómo su naturaleza a través de los kernels mejora el rendimiento drásticamente.\n",
    "\n",
    "Sin embargo, el dataset de Fashion MNIST es un dataset bastante irreal en términos de espacio y de tamaño para tareas de visión artificial reales. Una de las limitaciones más grandes al entrenar redes convolucionales es que no siempre se puede cargar todo el dataset a la memoria RAM. La memoria RAM es un recurso limitado en cualquier servidor, por lo que se debe buscar una forma más eficiente de poder realizar este tipo de tareas.\n",
    "\n",
    "En este notebook exploraremos el caso con un dataset más cercano a lo que un ingeniero se puede encontrar *en la vida real* y cómo tratar con el mismo,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El dataset: Humanos vs Caballos\n",
    "Usaremos un dataset preparado por Laurence Moroney de Google el cual contiene imágenes sintéticas de caballos y personas. Primero tenemos que descargar tanto el conjunto de entrenamiento como el conjunto de validación o pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXZT2UsyIVe_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate \\\n",
    "#     https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip \\\n",
    "#     -O data/horse-or-human.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mLij6qde6Ox",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate \\\n",
    "#     https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip \\\n",
    "#     -O data/validation-horse-or-human.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9brUxyTpYZHy"
   },
   "source": [
    "En la siguiente celda usaremos funciones de la librería os, para acceder a librerías del sistema operativo de tal manera que podamos acceder al sistema de archivos de la máquina.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLy3pthUS0D2"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import zipfile\n",
    "\n",
    "# local_zip = 'data/horse-or-human.zip'\n",
    "# zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "# zip_ref.extractall('data/horse-or-human')\n",
    "# local_zip = 'data/validation-horse-or-human.zip'\n",
    "# zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "# zip_ref.extractall('data/validation-horse-or-human')\n",
    "# zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-qUPyfO7Qr8"
   },
   "source": [
    "Los contenidos de los archivos comprimidos .zip se extraerán en el directorio base /tmp/horse-or-human, dentro del mismo se tendrán 2 subdirectorios: `horse` y `human`\n",
    "\n",
    "El **conjunto de entrenamiento** corresponde a los datos que se usan para decirle a la red neuronal 'así se ve un caballo'o 'así se ve una persona'.\n",
    "\n",
    "Algo que considerar es que no estamos etiquetando explícitamente las imágenes como caballos o personas. Esto se logrará usando un dataset especial de Pytorch que es capaz de usar la estructura de nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n",
    "\n",
    "\n",
    "# Define the necessary transformations\n",
    "preprocess = Compose([\n",
    "    ToTensor(),\n",
    "    Resize((300, 300)),\n",
    "    Resize((300, 300)),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(root=\"data/horse-or-human\", transform=preprocess)\n",
    "test_dataset = ImageFolder(root=\"data/validation-horse-or-human\", transform=preprocess)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027 256\n",
      "torch.Size([3, 300, 300])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(test_dataset))\n",
    "print(train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oqBkNBJmtUv"
   },
   "source": [
    "## Definiendo el modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando: cuda\n"
     ]
    }
   ],
   "source": [
    "# dispositivo de entrenamiento\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Usando: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qvfZg3LQbD-5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (backbone): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=32768, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=32768, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        # flatten tensor\n",
    "        flat_features = features.view(features.size(0), -1)\n",
    "        # apply the classifier\n",
    "        logits = self.head(flat_features)\n",
    "        return logits\n",
    "    \n",
    "model = ConvNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnhYCP4tdqjC"
   },
   "source": [
    "Agregamos las capas convolucionales similar al ejemplo anterior, y *aplanamos* el resultado final para continuar con las capas densamente conectadas.\n",
    "\n",
    "Seguidamente añadimos las capas densamente conectadas.\n",
    "\n",
    "Nótese que debido a que usamos un problema de clasificación binaria, la salida de nuestra red neuronal será una función [*sigmoide*](https://wikipedia.org/wiki/Sigmoid_function), de tal manera que la salida ser'a un escalar entre 0 y 1, se puede interpretar este valor como una probabilidad de que la imagen pertenezca a la clase 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucle de entrenamiento\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # modo entrenamiento\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # mover si es necesario\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # forward\n",
    "        # prediccion y costo\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        loss, current = loss.item(), batch * batch_size + len(X)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# funcion de pruebas\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # modo evaluacion\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    # evaluacion del modelo con torch.no_grad\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mu3Jdwkjwax4"
   },
   "source": [
    "### Entrenamiento\n",
    "Se entrenará la red por 15 épocas. Esto puede tomar algunos minutos en completarse.\n",
    "\n",
    "Monitoree los valores en cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.700466  [   64/ 1027]\n",
      "loss: 1.301757  [  128/ 1027]\n",
      "loss: 0.667960  [  192/ 1027]\n",
      "loss: 0.689917  [  256/ 1027]\n",
      "loss: 0.700219  [  320/ 1027]\n",
      "loss: 0.674855  [  384/ 1027]\n",
      "loss: 0.692324  [  448/ 1027]\n",
      "loss: 0.687158  [  512/ 1027]\n",
      "loss: 0.663003  [  576/ 1027]\n",
      "loss: 0.615796  [  640/ 1027]\n",
      "loss: 0.592397  [  704/ 1027]\n",
      "loss: 0.502912  [  768/ 1027]\n",
      "loss: 0.575882  [  832/ 1027]\n",
      "loss: 0.526264  [  896/ 1027]\n",
      "loss: 0.291210  [  960/ 1027]\n",
      "loss: 0.723424  [ 1024/ 1027]\n",
      "loss: 0.461650  [ 1027/ 1027]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pepe/miniconda3/envs/cv_unifranz/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 1.069893 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.485712  [   64/ 1027]\n",
      "loss: 0.461571  [  128/ 1027]\n",
      "loss: 0.381999  [  192/ 1027]\n",
      "loss: 0.392685  [  256/ 1027]\n",
      "loss: 0.284641  [  320/ 1027]\n",
      "loss: 0.491194  [  384/ 1027]\n",
      "loss: 0.265816  [  448/ 1027]\n",
      "loss: 0.213422  [  512/ 1027]\n",
      "loss: 0.202699  [  576/ 1027]\n",
      "loss: 0.176199  [  640/ 1027]\n",
      "loss: 0.222959  [  704/ 1027]\n",
      "loss: 0.105857  [  768/ 1027]\n",
      "loss: 0.194736  [  832/ 1027]\n",
      "loss: 0.133967  [  896/ 1027]\n",
      "loss: 0.215253  [  960/ 1027]\n",
      "loss: 0.089432  [ 1024/ 1027]\n",
      "loss: 0.003691  [ 1027/ 1027]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 2.591246 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.117814  [   64/ 1027]\n",
      "loss: 0.080037  [  128/ 1027]\n",
      "loss: 0.045398  [  192/ 1027]\n",
      "loss: 0.171181  [  256/ 1027]\n",
      "loss: 0.081916  [  320/ 1027]\n",
      "loss: 0.091020  [  384/ 1027]\n",
      "loss: 0.043166  [  448/ 1027]\n",
      "loss: 0.051927  [  512/ 1027]\n",
      "loss: 0.089662  [  576/ 1027]\n",
      "loss: 0.130286  [  640/ 1027]\n",
      "loss: 0.060376  [  704/ 1027]\n",
      "loss: 0.069248  [  768/ 1027]\n",
      "loss: 0.470126  [  832/ 1027]\n",
      "loss: 0.299539  [  896/ 1027]\n",
      "loss: 0.149808  [  960/ 1027]\n",
      "loss: 0.140345  [ 1024/ 1027]\n",
      "loss: 0.066505  [ 1027/ 1027]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.889497 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.018793  [   64/ 1027]\n",
      "loss: 0.437758  [  128/ 1027]\n",
      "loss: 0.153888  [  192/ 1027]\n",
      "loss: 0.140191  [  256/ 1027]\n",
      "loss: 0.195318  [  320/ 1027]\n",
      "loss: 0.089474  [  384/ 1027]\n",
      "loss: 0.149369  [  448/ 1027]\n",
      "loss: 0.261005  [  512/ 1027]\n",
      "loss: 0.267468  [  576/ 1027]\n",
      "loss: 0.170443  [  640/ 1027]\n",
      "loss: 0.126853  [  704/ 1027]\n",
      "loss: 0.120445  [  768/ 1027]\n",
      "loss: 0.105996  [  832/ 1027]\n",
      "loss: 0.143661  [  896/ 1027]\n",
      "loss: 0.095993  [  960/ 1027]\n",
      "loss: 0.056846  [ 1024/ 1027]\n",
      "loss: 0.006026  [ 1027/ 1027]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 1.332739 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.042897  [   64/ 1027]\n",
      "loss: 0.094922  [  128/ 1027]\n",
      "loss: 0.035682  [  192/ 1027]\n",
      "loss: 0.015854  [  256/ 1027]\n",
      "loss: 0.146246  [  320/ 1027]\n",
      "loss: 0.006456  [  384/ 1027]\n",
      "loss: 0.013728  [  448/ 1027]\n",
      "loss: 0.105205  [  512/ 1027]\n",
      "loss: 0.065104  [  576/ 1027]\n",
      "loss: 0.034795  [  640/ 1027]\n",
      "loss: 0.031382  [  704/ 1027]\n",
      "loss: 0.000870  [  768/ 1027]\n",
      "loss: 0.025049  [  832/ 1027]\n",
      "loss: 0.005381  [  896/ 1027]\n",
      "loss: 0.023818  [  960/ 1027]\n",
      "loss: 0.013483  [ 1024/ 1027]\n",
      "loss: 0.000020  [ 1027/ 1027]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 3.594548 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.027966  [   64/ 1027]\n",
      "loss: 0.058809  [  128/ 1027]\n",
      "loss: 0.053203  [  192/ 1027]\n",
      "loss: 0.038139  [  256/ 1027]\n",
      "loss: 0.019243  [  320/ 1027]\n",
      "loss: 0.053894  [  384/ 1027]\n",
      "loss: 0.001568  [  448/ 1027]\n",
      "loss: 0.013807  [  512/ 1027]\n",
      "loss: 0.067662  [  576/ 1027]\n",
      "loss: 0.001700  [  640/ 1027]\n",
      "loss: 0.047561  [  704/ 1027]\n",
      "loss: 0.002698  [  768/ 1027]\n",
      "loss: 0.008030  [  832/ 1027]\n",
      "loss: 0.013616  [  896/ 1027]\n",
      "loss: 0.119866  [  960/ 1027]\n",
      "loss: 0.012447  [ 1024/ 1027]\n",
      "loss: 0.000007  [ 1027/ 1027]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 1.438193 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.004007  [   64/ 1027]\n",
      "loss: 0.047397  [  128/ 1027]\n",
      "loss: 0.046432  [  192/ 1027]\n",
      "loss: 0.007203  [  256/ 1027]\n",
      "loss: 0.002515  [  320/ 1027]\n",
      "loss: 0.016071  [  384/ 1027]\n",
      "loss: 0.005996  [  448/ 1027]\n",
      "loss: 0.001187  [  512/ 1027]\n",
      "loss: 0.000715  [  576/ 1027]\n",
      "loss: 0.007929  [  640/ 1027]\n",
      "loss: 0.017571  [  704/ 1027]\n",
      "loss: 0.017796  [  768/ 1027]\n",
      "loss: 0.067907  [  832/ 1027]\n",
      "loss: 0.002220  [  896/ 1027]\n",
      "loss: 0.023575  [  960/ 1027]\n",
      "loss: 0.001729  [ 1024/ 1027]\n",
      "loss: 0.000066  [ 1027/ 1027]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 4.628676 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.094499  [   64/ 1027]\n",
      "loss: 0.030332  [  128/ 1027]\n",
      "loss: 0.005488  [  192/ 1027]\n",
      "loss: 0.014433  [  256/ 1027]\n",
      "loss: 0.001106  [  320/ 1027]\n",
      "loss: 0.007514  [  384/ 1027]\n",
      "loss: 0.008062  [  448/ 1027]\n",
      "loss: 0.007806  [  512/ 1027]\n",
      "loss: 0.000272  [  576/ 1027]\n",
      "loss: 0.000072  [  640/ 1027]\n",
      "loss: 0.011418  [  704/ 1027]\n",
      "loss: 0.001362  [  768/ 1027]\n",
      "loss: 0.001919  [  832/ 1027]\n",
      "loss: 0.000802  [  896/ 1027]\n",
      "loss: 0.061067  [  960/ 1027]\n",
      "loss: 0.000163  [ 1024/ 1027]\n",
      "loss: 0.000191  [ 1027/ 1027]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 3.635403 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.000260  [   64/ 1027]\n",
      "loss: 0.002625  [  128/ 1027]\n",
      "loss: 0.010700  [  192/ 1027]\n",
      "loss: 0.005488  [  256/ 1027]\n",
      "loss: 0.004527  [  320/ 1027]\n",
      "loss: 0.018730  [  384/ 1027]\n",
      "loss: 0.001172  [  448/ 1027]\n",
      "loss: 0.000782  [  512/ 1027]\n",
      "loss: 0.002370  [  576/ 1027]\n",
      "loss: 0.000751  [  640/ 1027]\n",
      "loss: 0.001283  [  704/ 1027]\n",
      "loss: 0.013474  [  768/ 1027]\n",
      "loss: 0.001286  [  832/ 1027]\n",
      "loss: 0.000478  [  896/ 1027]\n",
      "loss: 0.000551  [  960/ 1027]\n",
      "loss: 0.005330  [ 1024/ 1027]\n",
      "loss: 0.000000  [ 1027/ 1027]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 4.031040 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.000913  [   64/ 1027]\n",
      "loss: 0.014265  [  128/ 1027]\n",
      "loss: 0.001180  [  192/ 1027]\n",
      "loss: 0.007703  [  256/ 1027]\n",
      "loss: 0.000097  [  320/ 1027]\n",
      "loss: 0.000268  [  384/ 1027]\n",
      "loss: 0.000360  [  448/ 1027]\n",
      "loss: 0.000039  [  512/ 1027]\n",
      "loss: 0.000387  [  576/ 1027]\n",
      "loss: 0.015038  [  640/ 1027]\n",
      "loss: 0.003156  [  704/ 1027]\n",
      "loss: 0.000198  [  768/ 1027]\n",
      "loss: 0.000053  [  832/ 1027]\n",
      "loss: 0.009233  [  896/ 1027]\n",
      "loss: 0.000191  [  960/ 1027]\n",
      "loss: 0.131561  [ 1024/ 1027]\n",
      "loss: 0.000001  [ 1027/ 1027]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 3.286307 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# bucle principal\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6vSHzPR2ghH"
   },
   "source": [
    "### Haciendo predicciones\n",
    "\n",
    "Vamos a usar el modelo entrenado haciendo predicciones. La celda a continuación nos permitirá subir uno o más archivos para realizar predicciones sobre los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(534, 600, 3)\n",
      "torch.Size([1, 3, 300, 300])\n",
      "prediction: tensor([[ 0.2308, -2.6987]], device='cuda:0'), 0\n",
      " is a horse\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Resize\n",
    "# %matplotlib.inline\n",
    "\n",
    "\n",
    "\n",
    "# img = \n",
    "# img = cv2.resize(img, (150, 150))\n",
    "img = cv2.cvtColor(cv2.imread(\"caballo2.jpg\"), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(img.shape)\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "x = preprocess(img)\n",
    "x = x.unsqueeze(0)\n",
    "x = x.to(device)\n",
    "# x = Resize((150, 15))(x)\n",
    "print(x.shape)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "print(f\"prediction: {pred}, {pred.argmax(1).item()}\")\n",
    "# print(pred.ar)\n",
    "if pred.argmax(1).item()==1:\n",
    "    print(\" is a human\")\n",
    "else:\n",
    "    print(\" is a horse\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8EHQyWGDvWz"
   },
   "source": [
    "### Visualizando Representaciones Internas\n",
    "\n",
    "Tambien es divertido visualizar qué tipo de filtros y qué tipo de características nuestra red ha aprendido a resaltar.\n",
    "\n",
    "Seleccionando una imagen aleatoria en el conjunto de entrenamiento, se puede generar una figura con la salida de cada capa convolucional representando a cada filtro y su correspondiente mapa de características, las mismas se suelen llamar **representaciones internas** de la red neuronal.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Course 1 - Part 8 - Lesson 4 - Notebook.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "62eb3e910e44ea0e9978ea29c6f3fc7540fb99bfa181faf1a80d42ed442aa249"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('unifranz': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
